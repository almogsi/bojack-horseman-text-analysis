---
output: md_document
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## New Season of Bojack Horseman - NO SPOILERS!

A new season of my favorite show, Bojack Horseman, just dropped on Netflix, and I have absolutely zero time to sit down and watch it. However, in some magical way there was time for me to write this blog post and do this analysis. What analysis you ask? I'll tell you in a minute, I ain't Horsin' Around.

Bojack Horseman, in case you got stuck in 2013, is an animated comedy-drama about a washed up '90s sitcom star, who is also a horse (animals are anthropomorphized in the show's universe). If you follow Bojack, you know that it isn't your everyday animated series. It's deep, cynical and lights up some of the darkest corners of damaged human behavior. Moreover, it keeps getting better, or should I say - heavier.

The psychologist in me got triggered, if indeed Bojack is getting deeper and deeper, perhaps this pattern could be observed in the show's script? I mean, would the show runners use more *abstract* language compared to *concrete* language as the show progresses?
The data scientist in me said "I'm on it".

## Scrape Bojack Script and Language Analysis

First, I had to scrape Bojack's script. I'll do it the `rvest`, `xml2` and `stringer` packages.
It's actually my first doing web scarping so apologies for the non-elegant code

```{r, warning=F, message=F}
library(xml2)
library(rvest)
library(stringr)

#data frame pre-allocation
m <- matrix(nrow = 5*10, ncol = 3)
colnames(m) <- c("season", "episode", "text")
m <- as.data.frame(m)

count = 1
for (i in 1:5){
  for (j in 1:10){
    if (j<10){
      scrappedurl <-paste0("https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=bojack-horseman-2014&episode=s0",
                           i, "e0", j)
      html.raw <- read_html(scrappedurl)
      n <- html_nodes(html.raw, "div#content_container")
      txt <- html_text(n)
      txt <- str_replace_all(txt, "[\r\n\t]" , "")
      t <- str_split(txt, "Episode Script", simplify = T)[3]
      m$season[count] <- i
      m$episode[count] <- j
      m$text[count] <- t
    }
    else{
      scrappedurl <-paste0("https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=bojack-horseman-2014&episode=s0",
                           i, "e", j)
      html.raw <- read_html(scrappedurl)
      n <- html_nodes(html.raw, "div#content_container")
      txt <- html_text(n)
      txt <- str_replace_all(txt, "[\r\n\t]" , "")
      t <- str_split(txt, "Episode Script", simplify = T)[3]
      m$season[count] <- i
      m$episode[count] <- j
      m$text[count] <- t
    }
    count <-  count+1
  }
}

library(kableExtra)
#let's see the dataframe
kable(m) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Right, so I've scrapped the script from  [here](https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=bojack-horseman-2014) and put it all in a dataframe. Next step is analyze the content. To do that, I'm using Concreteness-Abstractness norms by Brysbaert, Warriner, & Kuperman [(2014)](https://link.springer.com/article/10.3758/s13428-013-0403-5). I've wrapped these norms (and other) in a little primitive package called `TATE`.

```{r, warning=F, message=F}
#devtools::install_github("almogsi/TATE")
library(tidyverse)
library(TATE)
m <- m %>%
  mutate(id = 1:50) %>%
  group_by(id) %>%
  mutate(concrete = concretness(text))

#let's look at the concretness ratings by episode
library(extrafont)
loadfonts()
theme_set(theme_light(base_size = 15, base_family = "Candara"))
g <- ggplot(m) +
  geom_line(aes(id, concrete)) + scale_x_continuous(breaks = seq(5,50,5)) + 
  geom_smooth(aes(id, concrete), method = "lm") + 
  labs(x = "Episode id and use a pretty font", y = "Concreteness Score") +
  theme(legend.position = "none",
        axis.title = element_text(size = 12),
        axis.text.x = element_text(family = "Candara", size = 10),
        panel.grid = element_blank())

g
```

This is cool, let's add some cool annotation (shout out to [CÃ©dric Scherer](https://cedricscherer.netlify.com/2019/05/17/the-evolution-of-a-ggplot-ep.-1/)) 

```{r, warning=F, message=F}
g <- g + ggplot2::annotate("text", x = 25, y = 3.25, family = "Candara", size = 2.7, color = "gray20",
           label = glue::glue("s03e04 'Fish Out of Water'")) +
  ggplot2::annotate("text", x = 44, y = 2.9, family = "Candara", size = 2.7, color = "gray20",
           label = glue::glue("s05e07 'INT. SUB'"))

arrows <- tibble(
  x1 = c(25.0, 44),
  x2 = c(24, 47),
  y1 = c(3.24, 2.915),
  y2 = c(3.22, 2.937)
)

g + geom_curve(data = arrows, aes(x = x1, y = y1, xend = x2, yend = y2),
             arrow = arrow(length = unit(0.07, "inch")), size = 0.4,
             color = "gray20", curvature = -0.3)

cor.test(m$id, m$concrete, alternative = "less")
```

There's definite a negative trend here, but it's not super robust. My take is that we're a little underpowered. Would this trend carry on to the next season?
<center>
![alt text](https://media.giphy.com/media/3osxYlhbYjOIDEZhkI/giphy.gif "Todd <3") 
</center>

Next, we can look at the what type of language is associated with each season. I'll be comparing season 1 (in blue) with season 5 (red) using `tidytext` `tm`, and `wordcloud` packages . Ready?

```{r, warning=F, message=F}
library(tidytext)
library(tm)
season_01 <- m %>%
  ungroup() %>%
  filter(season==1) %>%
  mutate(clean_text = removeNumbers(text),
         clean_text = tolower(clean_text),
         clean_text = removePunctuation(clean_text),
         clean_text = stripWhitespace(clean_text)) %>%
  unnest_tokens(word,clean_text) %>%
  anti_join(get_stopwords()) %>%
  count(word, sort=T) %>%
  mutate(s01.prop = n/sum(n))

season_05 <- m %>%
  ungroup() %>%
  filter(season==5) %>%
  mutate(clean_text = removeNumbers(text),
         clean_text = tolower(clean_text),
         clean_text = removePunctuation(clean_text),
         clean_text = stripWhitespace(clean_text)) %>%
  unnest_tokens(word,clean_text) %>%
  anti_join(get_stopwords()) %>%
  count(word, sort=T) %>%
  mutate(s05.prop = n/sum(n))

merged <- full_join(season_01, season_05, by = "word") %>%
  mutate(s01.prop = ifelse(is.na(s01.prop), 0, s01.prop),
         s05.prop = ifelse(is.na(s05.prop), 0, s05.prop)) %>%
  mutate(diff = s01.prop-s05.prop,
         color = ifelse(diff>0, "blue", "red"),
         abs = abs(diff)) %>%
  arrange(desc(abs))


#plot word cloud to visualize differences
library(wordcloud)
wordcloud(words = merged$word, freq = merged$abs, min.freq =0,
          max.words=40, random.order=FALSE, rot.per=0.35, 
          colors=merged$color, ordered.colors=TRUE)

```

Alright I'm gonna watch the new season right no, for anything other than spoilers, I'm on [twitter](https://twitter.com/almogsi). Bye!

<center>
![alt text](https://media.giphy.com/media/UdhjRiLkfuBd6/giphy.gif "It get's easier") 
</center>


